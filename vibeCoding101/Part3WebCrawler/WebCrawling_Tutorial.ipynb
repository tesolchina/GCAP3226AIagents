{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b644a8ac",
   "metadata": {},
   "source": [
    "# Web Crawling for Policy Analysis\n",
    "## GCAP3226: Empowering Citizens through Data\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Understand ethical web crawling practices\n",
    "2. Analyze robots.txt and sitemaps\n",
    "3. Identify pages with quantitative data\n",
    "4. Extract and organize data from government websites\n",
    "5. Apply these techniques to policy research\n",
    "\n",
    "**Case Study:** Cyberdefender.hk - Hong Kong Government Cybersecurity Portal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2067ed4d",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Web Crawling Ethics\n",
    "\n",
    "### What is robots.txt?\n",
    "The `robots.txt` file tells web crawlers which parts of a website they can access. It's a fundamental part of web crawling ethics.\n",
    "\n",
    "**Key Rules:**\n",
    "- Always check robots.txt before crawling\n",
    "- Respect the directives (Disallow, Allow)\n",
    "- Implement rate limiting to avoid overloading servers\n",
    "- Identify your crawler with a descriptive User-Agent\n",
    "\n",
    "Let's check cyberdefender.hk's robots.txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d104c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Target website\n",
    "BASE_URL = \"https://cyberdefender.hk\"\n",
    "\n",
    "# Fetch robots.txt\n",
    "robots_url = f\"{BASE_URL}/robots.txt\"\n",
    "response = requests.get(robots_url)\n",
    "print(\"robots.txt content:\")\n",
    "print(\"=\" * 50)\n",
    "print(response.text)\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n✓ Analysis: No crawling restrictions! (Disallow: is empty)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c615736",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Check robots.txt for other government websites\n",
    "\n",
    "Try checking robots.txt for these Hong Kong government sites:\n",
    "- https://www.info.gov.hk\n",
    "- https://www.censtatd.gov.hk (Census and Statistics Department)\n",
    "- https://www.epd.gov.hk (Environmental Protection Department)\n",
    "\n",
    "**Question:** Do they all allow crawling? Are there any restrictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b285f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Try fetching robots.txt from the sites above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edf586e",
   "metadata": {},
   "source": [
    "## Part 2: Discovering Content with Sitemaps\n",
    "\n",
    "Sitemaps are XML files that list all pages on a website. They make crawling much more efficient!\n",
    "\n",
    "### Sitemap Index\n",
    "Large websites often have a `sitemap_index.xml` that points to multiple sitemaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372083ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch sitemap index\n",
    "sitemap_index_url = f\"{BASE_URL}/sitemap_index.xml\"\n",
    "response = requests.get(sitemap_index_url)\n",
    "\n",
    "# Parse XML\n",
    "root = ET.fromstring(response.content)\n",
    "namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "\n",
    "# Extract sitemap URLs\n",
    "sitemaps = []\n",
    "for sitemap in root.findall('.//ns:sitemap', namespace):\n",
    "    loc = sitemap.find('ns:loc', namespace)\n",
    "    lastmod = sitemap.find('ns:lastmod', namespace)\n",
    "    if loc is not None:\n",
    "        sitemaps.append({\n",
    "            'url': loc.text,\n",
    "            'last_modified': lastmod.text if lastmod is not None else 'N/A',\n",
    "            'name': loc.text.split('/')[-1]\n",
    "        })\n",
    "\n",
    "# Display as DataFrame\n",
    "df_sitemaps = pd.DataFrame(sitemaps)\n",
    "print(f\"Found {len(sitemaps)} sitemaps:\\n\")\n",
    "print(df_sitemaps.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70618b63",
   "metadata": {},
   "source": [
    "### Key Observation\n",
    "Notice the `sdm_downloads-sitemap.xml` - this contains downloadable files, which often include quantitative data like reports, statistics, and datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29c066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch and parse a sitemap\n",
    "def fetch_sitemap(sitemap_url):\n",
    "    \"\"\"Fetch and parse a sitemap XML file\"\"\"\n",
    "    response = requests.get(sitemap_url)\n",
    "    root = ET.fromstring(response.content)\n",
    "    namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "    \n",
    "    urls = []\n",
    "    for url_elem in root.findall('.//ns:url', namespace):\n",
    "        loc = url_elem.find('ns:loc', namespace)\n",
    "        if loc is not None:\n",
    "            urls.append(loc.text)\n",
    "    \n",
    "    return urls\n",
    "\n",
    "# Fetch all URLs from all sitemaps\n",
    "all_urls = {}\n",
    "for sitemap in sitemaps:\n",
    "    time.sleep(1)  # Rate limiting\n",
    "    urls = fetch_sitemap(sitemap['url'])\n",
    "    all_urls[sitemap['name']] = urls\n",
    "    print(f\"{sitemap['name']}: {len(urls)} URLs\")\n",
    "\n",
    "total_urls = sum(len(urls) for urls in all_urls.values())\n",
    "print(f\"\\nTotal URLs to crawl: {total_urls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8c1a8b",
   "metadata": {},
   "source": [
    "## Part 3: Detecting Quantitative Data\n",
    "\n",
    "Not all pages have useful quantitative data. We need to be selective!\n",
    "\n",
    "### Detection Criteria:\n",
    "1. **Numbers:** Percentages, currency, statistics\n",
    "2. **Tables:** Structured data\n",
    "3. **Keywords:** \"statistics\", \"data\", \"analysis\", \"report\", \"survey\"\n",
    "4. **Charts/Graphs:** Visual data representations\n",
    "5. **Downloadable Files:** PDFs, Excel files, CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcf2b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_quantitative_data(html_content, url):\n",
    "    \"\"\"\n",
    "    Analyze HTML content to detect quantitative data\n",
    "    Returns: (has_data, metadata)\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    metadata = {\n",
    "        'url': url,\n",
    "        'has_numbers': False,\n",
    "        'has_tables': False,\n",
    "        'has_charts': False,\n",
    "        'number_count': 0,\n",
    "        'table_count': 0,\n",
    "        'keywords': []\n",
    "    }\n",
    "    \n",
    "    # 1. Count meaningful numbers\n",
    "    number_patterns = [\n",
    "        r'\\d+(?:\\.\\d+)?%',  # Percentages\n",
    "        r'\\$\\d+(?:,\\d{3})*',  # Currency\n",
    "        r'HK\\$\\d+(?:,\\d{3})*',  # HK currency\n",
    "        r'\\d{1,3}(?:,\\d{3})+',  # Numbers with commas\n",
    "    ]\n",
    "    \n",
    "    for pattern in number_patterns:\n",
    "        matches = re.findall(pattern, text)\n",
    "        metadata['number_count'] += len(matches)\n",
    "    \n",
    "    if metadata['number_count'] > 5:\n",
    "        metadata['has_numbers'] = True\n",
    "    \n",
    "    # 2. Check for tables\n",
    "    tables = soup.find_all('table')\n",
    "    metadata['table_count'] = len(tables)\n",
    "    metadata['has_tables'] = len(tables) > 0\n",
    "    \n",
    "    # 3. Check for keywords\n",
    "    keywords = ['statistics', 'data', 'analysis', 'report', 'survey',\n",
    "                'percentage', 'total', 'average', 'rate']\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        if keyword.lower() in text.lower():\n",
    "            metadata['keywords'].append(keyword)\n",
    "    \n",
    "    # 4. Check for charts\n",
    "    chart_indicators = ['chart', 'graph', 'diagram']\n",
    "    for indicator in chart_indicators:\n",
    "        if indicator.lower() in text.lower():\n",
    "            metadata['has_charts'] = True\n",
    "            break\n",
    "    \n",
    "    # Determine if page has quantitative data\n",
    "    has_data = (\n",
    "        metadata['has_numbers'] or \n",
    "        metadata['has_tables'] or\n",
    "        (metadata['has_charts'] and len(metadata['keywords']) > 0)\n",
    "    )\n",
    "    \n",
    "    return has_data, metadata\n",
    "\n",
    "print(\"✓ Quantitative data detection function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1912a663",
   "metadata": {},
   "source": [
    "### Example: Test the Detection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd816e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample page\n",
    "test_url = all_urls['post-sitemap.xml'][0] if 'post-sitemap.xml' in all_urls else BASE_URL\n",
    "\n",
    "print(f\"Testing URL: {test_url}\\n\")\n",
    "response = requests.get(test_url)\n",
    "has_data, metadata = detect_quantitative_data(response.content, test_url)\n",
    "\n",
    "print(f\"Has quantitative data: {has_data}\")\n",
    "print(f\"\\nDetection details:\")\n",
    "print(f\"  Numbers found: {metadata['number_count']}\")\n",
    "print(f\"  Tables found: {metadata['table_count']}\")\n",
    "print(f\"  Has charts: {metadata['has_charts']}\")\n",
    "print(f\"  Keywords: {', '.join(metadata['keywords'][:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fa087c",
   "metadata": {},
   "source": [
    "## Part 4: Implementing Ethical Crawling\n",
    "\n",
    "### Best Practices:\n",
    "1. **Rate Limiting:** Wait between requests (1-2 seconds)\n",
    "2. **User-Agent:** Identify your crawler\n",
    "3. **Error Handling:** Handle 404s, timeouts gracefully\n",
    "4. **Logging:** Track what you're doing\n",
    "5. **Respect robots.txt:** Always follow the rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c429f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_with_ethics(urls, max_pages=10):\n",
    "    \"\"\"\n",
    "    Ethically crawl a list of URLs\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'GCAP3226-Student-Crawler/1.0 (Educational Purpose)'\n",
    "    })\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, url in enumerate(urls[:max_pages], 1):\n",
    "        print(f\"[{i}/{min(len(urls), max_pages)}] Crawling: {url}\")\n",
    "        \n",
    "        try:\n",
    "            response = session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            has_data, metadata = detect_quantitative_data(response.content, url)\n",
    "            results.append({\n",
    "                'url': url,\n",
    "                'has_data': has_data,\n",
    "                'numbers': metadata['number_count'],\n",
    "                'tables': metadata['table_count'],\n",
    "                'keywords': ', '.join(metadata['keywords'][:3])\n",
    "            })\n",
    "            \n",
    "            if has_data:\n",
    "                print(f\"  ✓ Found quantitative data!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {str(e)}\")\n",
    "            results.append({\n",
    "                'url': url,\n",
    "                'has_data': False,\n",
    "                'numbers': 0,\n",
    "                'tables': 0,\n",
    "                'keywords': f'Error: {str(e)}'\n",
    "            })\n",
    "        \n",
    "        # Rate limiting - wait 2 seconds between requests\n",
    "        time.sleep(2)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"✓ Ethical crawler function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a072af",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Crawl Sample Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff9f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl first 5 pages from the posts sitemap\n",
    "sample_urls = all_urls.get('post-sitemap.xml', [])[:5]\n",
    "\n",
    "print(f\"Crawling {len(sample_urls)} sample pages...\\n\")\n",
    "results_df = crawl_with_ethics(sample_urls, max_pages=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nPages with data: {results_df['has_data'].sum()} / {len(results_df)}\")\n",
    "print(f\"Success rate: {results_df['has_data'].sum() / len(results_df) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafb3fe5",
   "metadata": {},
   "source": [
    "## Part 5: Comparing with Google Site-Specific Search\n",
    "\n",
    "Let's compare our crawler with Google's site-specific search.\n",
    "\n",
    "### Google Site Search Syntax:\n",
    "```\n",
    "site:cyberdefender.hk \"statistics\" OR \"data\" OR \"report\"\n",
    "```\n",
    "\n",
    "### Advantages of Crawler:\n",
    "1. **Systematic:** Covers all pages via sitemap\n",
    "2. **Programmable:** Automated data extraction\n",
    "3. **Customizable:** Your own detection criteria\n",
    "4. **Downloadable:** Save pages and files locally\n",
    "5. **Reproducible:** Can rerun with same parameters\n",
    "\n",
    "### Advantages of Google Search:\n",
    "1. **Fast:** Instant results\n",
    "2. **Smart:** Better keyword matching\n",
    "3. **No coding:** User-friendly interface\n",
    "4. **No restrictions:** Google handles rate limiting\n",
    "\n",
    "**Conclusion:** Use both! Google for quick discovery, crawlers for systematic data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd46b6f8",
   "metadata": {},
   "source": [
    "## Part 6: Your Assignment - Crawl a Government Website\n",
    "\n",
    "Choose one of these Hong Kong government websites and apply what you've learned:\n",
    "\n",
    "1. **Census and Statistics Department:** https://www.censtatd.gov.hk\n",
    "   - Rich in quantitative data\n",
    "   - Population, economy, social indicators\n",
    "\n",
    "2. **Environmental Protection Department:** https://www.epd.gov.hk\n",
    "   - Air quality data\n",
    "   - Waste statistics\n",
    "   - Environmental reports\n",
    "\n",
    "3. **Transport Department:** https://www.td.gov.hk\n",
    "   - Traffic statistics\n",
    "   - Public transport data\n",
    "   - Road safety reports\n",
    "\n",
    "4. **Food and Health Bureau:** https://www.fhb.gov.hk\n",
    "   - Healthcare statistics\n",
    "   - Disease surveillance data\n",
    "   - Hospital data\n",
    "\n",
    "### Assignment Tasks:\n",
    "\n",
    "1. **Check robots.txt** - Are you allowed to crawl?\n",
    "2. **Find sitemap** - Does the site have a sitemap?\n",
    "3. **Identify target pages** - What pages likely have quantitative data?\n",
    "4. **Crawl ethically** - Implement rate limiting and proper User-Agent\n",
    "5. **Extract data** - Download pages and files with quantitative data\n",
    "6. **Analyze results** - What did you find? What data is useful for policy analysis?\n",
    "7. **Document** - Write a brief report on your findings\n",
    "\n",
    "### Deliverables:\n",
    "1. Python code (use the functions from this notebook)\n",
    "2. Data files collected\n",
    "3. Summary report (500-1000 words)\n",
    "4. Reflection on ethical considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512c26da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your assignment code here:\n",
    "# 1. Choose a government website\n",
    "# 2. Implement your crawler\n",
    "# 3. Collect and analyze data\n",
    "\n",
    "# Example starter code:\n",
    "# YOUR_TARGET_URL = \"https://www.censtatd.gov.hk\"  # Change this\n",
    "# \n",
    "# # Check robots.txt\n",
    "# response = requests.get(f\"{YOUR_TARGET_URL}/robots.txt\")\n",
    "# print(response.text)\n",
    "# \n",
    "# # Your code continues..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7658a67",
   "metadata": {},
   "source": [
    "## Part 7: Ethical Considerations and Best Practices\n",
    "\n",
    "### Legal and Ethical Framework:\n",
    "\n",
    "1. **Copyright:** Respect intellectual property rights\n",
    "2. **Privacy:** Don't collect personal data\n",
    "3. **Terms of Service:** Read and follow website terms\n",
    "4. **Server Load:** Don't overwhelm servers\n",
    "5. **Purpose:** Use data for legitimate research/analysis\n",
    "\n",
    "### Rate Limiting Guidelines:\n",
    "- Small sites: 2-3 seconds between requests\n",
    "- Medium sites: 1-2 seconds\n",
    "- Large sites (e.g., government): 0.5-1 second\n",
    "- **Never** faster than 0.5 seconds\n",
    "\n",
    "### When NOT to Crawl:\n",
    "- robots.txt explicitly disallows\n",
    "- Site requires login\n",
    "- Terms of service prohibit it\n",
    "- Data is available via API\n",
    "- Site is slow or unstable\n",
    "\n",
    "### Alternative: APIs\n",
    "Many government sites offer APIs for data access:\n",
    "- **Hong Kong Open Data:** https://data.gov.hk\n",
    "- Usually better than crawling!\n",
    "- Structured data, legal access, official support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9708eb4f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What You Learned:**\n",
    "1. ✓ How to check robots.txt and respect crawling rules\n",
    "2. ✓ How to discover content using sitemaps\n",
    "3. ✓ How to detect quantitative data in web pages\n",
    "4. ✓ How to implement ethical crawling practices\n",
    "5. ✓ How to compare crawler vs. Google search approaches\n",
    "6. ✓ How to apply these skills to government websites\n",
    "\n",
    "**Next Steps:**\n",
    "- Complete the assignment\n",
    "- Explore Hong Kong Open Data portal\n",
    "- Learn about web scraping libraries (Scrapy, Selenium)\n",
    "- Study data analysis techniques for policy research\n",
    "\n",
    "**Resources:**\n",
    "- [Web Scraping Best Practices](https://www.scrapehero.com/web-scraping-best-practices/)\n",
    "- [Hong Kong Open Data](https://data.gov.hk)\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/)\n",
    "- [Robots.txt Specification](https://www.robotstxt.org/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
