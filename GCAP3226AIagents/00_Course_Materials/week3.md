**Simon Wang:** The first thing is we got a WhatsApp group. And, please do join, because, this way we'll be able to interact more closely. If you haven't joined, please join. Anyone who is new, Just join… oh, okay, welcome. So you missed the first two weeks. You need to catch up a little bit, so, And also, I'm recording this, so anyone who wants to catch up and you can watch the video. So, as you can see, this is our Moodle classroom. We're still doing a little bit of paperwork. I think the course is already approved. It's just a matter of, some formality? We have a WhatsApp group here, please join if you haven't. And we have a Google Doc, and that's just basically for week 1 and Week 2. And then you can see, I'm trying to, rearranged, so it's reverse chronological order, so we have week one, there's on PDF… And then there's the… Week 2 and Week 3. And then there's also video recording from week 1 and Week 2. Alright? So, feel free to catch up here. And then, if you come to… this page… from… in-class and after-class discussion, so this is the Moodle forum. And once you join the… enter this page, you will be able to see today we're going to talk about the group project instruction and timeline. So… This project would be the… most important components in terms of assessment, because there's some other presentation, reflective essay, and human-AI collaboration notes. Those are the things that will build upon this core project that you have to form the groups and you need to work on. So that's why today, we're going to… Focus on talking about how to start your project. hopefully, I will finish in… half an hour, okay? Half an hour or something. And, in the meantime, you can start thinking about different possible topics. So, here, you can find a total of 8 topics. Alright? So, you can click into any of them and start exploring a little bit. If you are interested in any of them, you can leave a message. Another way to do this is to look around and see if there are any of your friends taking this course, anyone you know you want to form a team. You can form a team first, and then come here and explore topics. Okay, so either way will work. But the bottom line is, I hope, we hope, everybody will get a team and find a topic by the end of this week. Right? So, Because the first two weeks is, I mean, the first two weeks after the airdrop period, you need to start working on the project, and talia will also continue to introduce some tools, mass models, that you can use for this project. Okay. So, yeah, so please start looking at the topics, And, Also, take a look at the timeline. Okay, so can everybody… Find this page. This is the group project instruction and timeline. Moodle forum post. And, you should also be able to find this page. So this page is where you can find this post, as well as all the other topics. Alright, so just a show of hands, how many of you already got team members? you know who you want to work with. Raise your hand. Okay? The group size is 4 to 6. Okay? So, 4 to 6 is kind of a pretty flexible range here. if you want to form a smaller group, or… I don't think larger group will work. Smaller group, maybe you can talk to us. We will… You know, review and decide. Now, for those of you who do not know each other yet. You might want to leave a message to the WhatsApp group, or you want to introduce yourself, you want to say hi. Ideally, I hope each group would have at least one computer science or math major. Okay. this will kind of make life a bit easier, you know, and also the idea of offering this GE course is to practice and promote transdisciplinary thinking and teaching and learning. So, when we have someone from math or quantitative background, programming background, working with students from more qualitative background, I think that's exactly what we want, the kind of, combination. But of course, if you came to our course in week one and week 2, you can already experience the power of AI. So, you can think of AI as your partner, right? So even if you do not have the background in programming or quantitative reasoning, you know, you can always talk to AI. So, that being said… I would still encourage students to form teams that would have some team members from math and computer science, or science in general. So that's why maybe we should, because I think some students are still coming, so how about we just raise your hand if you are from computer science? Raise your hand. Hello! Tony, raise your hand if you're from computer science. Okay, Tony, and, so maybe we'll invite the computer science major to just introduce yourself a bit. But we have divided… we have a group already. You have a group already? Still, you can say hi. I'm Bonnie, and I'm a Year 4 Computer Science student, And… yes. Okay. And Tony? I'm totally… feel free to talk to me. Yeah, that's a good offer. Alright, so, both of you are in the WhatsApp group, right? Okay, anyone who is not in the WhatsApp group? Right? So what about others? Any math major? Okay, math major. Let's maybe say hi to everyone, and introduce yourself. Hi everyone, I'm Amy, and I'm here for math students. Okay. Alright, I'm sure there are other computer science and math majors in this course. Maybe, just, you can leave a message to the WhatsApp group, and, yeah, again, you know, just, hopefully you'll get to know each other and, and, form the groups. Alright, so if you recall what we discussed in the last two weeks. The purpose of this course is to synergize quantitative reasoning with qualitative reasoning. And our goal is to review how the government staff Practice data governance. Because we believe that decisions Made by the government staff should be informed by data. So that's the very basic premise. And starting point of this course. But we do not know to what extent the government is doing that. So that's why we're doing this project. Okay And we have a timeline. In the first, I mean, in the first two weeks after Eddrop, I always say it's two… Week 3 and week 4, we're going to explore the topics, all right? So you have the opportunity to look at different possibilities, you'll form the teams, and you'll learn a bit more about the math modeling, math tools that we're going to apply. And by week 5, we're going to finalize the topic, and in week 5, we're going to have small group meetings. Alright. Week 6 happens to be the public holiday, but that's when you can Both week 6 and week 7, you can start collecting more data. You need to send the request to the government staff. the specific department under the code on access to information. Alright, so that's the tool we're going to use. We call this secret weapon, because not many people are aware of this. So we're going to approach the specific government department and ask them questions about how they make decisions. We want to evaluate their decision-making process, and to see to what extent data has been used. My guess… Is that not much? Okay? So we're going to find out that the government staff are not really using that much of data, or taking this, what we call, data-driven approach. to making decisions. And then… We have to show them. How they can do a better job. Okay? So this is our mission of this course. We are going to look at the specific decisions made by the government. And after looking at what they are making the decision now, we are going to use the mass models, we're going to apply the mass models to some data. So we're gonna make an argument. The argument is that the government can do a better job by using data. This is actually in line with the chief executive's promise. You know, tomorrow, The chief executive will issue his new policy address. But if you look at the previous policy addresses, you can see that the chief executive promised to apply more data Okay, to use more data-driven approaches for governance. So our course is also helping the government to do what they promise to do. So, the first stage. The first thing you have to do is to select a specific decision made by the government. Okay? So one of the mistakes or pitfalls you want to avoid Is to just pick a broad topic. Without being able to narrow it down to something more specific. And the 8 topics that I offer you, in most topics, we already have something pretty concrete, except topic number 8. Topic number 8 is much more open-ended. Okay? So once you pick that specific policy area and a specific decision, you can then start reviewing different information documents… News media reports, you know, from the internet. And you will get a sense whether or not data-driven approach is being adopted. Okay. And then stage 2, you need to start collecting some data. There are multiple sources for such data collection. One important website you want to go to, you can visit right away, is data.gov.hk. Okay? Go to data.gov.hk, you will be able to see a lot of different quantitative data being provided by different government departments, or even some organizations or companies. That's the first source of data and information. The second source, as I mentioned earlier, we need to write to the government and request information. The deadline for sending that request is by week 6, because we have to wait for another 3 weeks before we get any substantive reply. Sometimes it can up to… can be up to 7 weeks. So, week 6, we send the government request of data, of information. That's how we will get a sense of how they actually apply the data-driven approach to decision-making. So by week 7, You know, you have a, holiday, In week 6? We should have a data inventory table, which means you got all the different data from different sources, it's all compiled, and you will be able to see a table. We should have sent all the requests. Of information to the government by email, total access to information. And, you can also do a little bit of fieldwork. Okay? So there are different forms of fieldwork you can do, just collect a little bit of information and data there as well. Field work is very important, because most likely. The government staff did not yet collect or curate all the data. That they're supposed to do, then we have to do a little bit of fieldwork to demonstrate this is actually feasible. Now, stage 3, after you have done all the exploration and all the data preparation, collection, now you will start analyzing the data, applying the mass models and tools As well as the critical thinking skills that we teach in this course. And this is why this course is co-hosted, mainly hosted by a maths department, but then co-taught by me from the Language Center. So, Dr. Wu, She's going to spend more time today As well as next week, in the coming weeks, to introduce the mass models. As well as some programming techniques, so we can visualize the data. So, the basic idea is that we are going to do a demo. To the government staff. Because you can't just write. An essay, or argument, and say, look, you have to apply or adopt data-driven approach to governance. You can't just say that. You have to make a convincing argument. How do you make a convincing argument? About this. We make a convincing argument By doing it, Okay? So, we're gonna show… to the lawmakers. And also the government staff that… It is feasible. and highly… Rewording. To apply the data, To apply the data-driven approach, to make decisions. And regrettably, The government staff haven't been doing that. Okay? So you can see, this argument can be very powerful. If we do it right, And we have all the channels, we have all the infrastructure. To disseminate that message. If we do a good job, To make a strong argument. I have connection with the lawmakers, I have different mechanisms To send that message to the lawmakers, the legislative councils, even to the Chief Executive's Office. Actually, I don't have any special connection. All I do is just email them. But we do also, published letters in South Carolina Morning Post. So, that's another way to… Make our voice heard. And then, after we build the argument, we make the recommendations, we will present. Okay, so we will have a presentation session. We're still talking with the lawmaker. We might run a session in the Legislative Council complex, so that's still to be announced. So I hope that makes sense. I mean, the whole post is pretty self-explanatory, so I'm… I won't get into all the details. Any questions? Any questions? Okay, good, we're good. Alright, so, very briefly, see, maybe 2 minutes per topic. Topic number 1. Actually, the first three topics are all about bus. Okay? Because, you know, somehow we got a lot of data, we got a lot of experience about buses, all right? So the first one is about the bus frequency, so how we can optimize bus frequency. There's a lot of trade-offs here. You know, ideally, we should increase the frequency as much as we can, right? But then you have to consider resources, and also, if there are too many buses on the road, there will be close traffic jams. You see what I mean? So, we need to optimize. We have to take into consideration different factors. Currently, because the franchise buses company, they got a franchise from the government, so the government, the transport department, actually have a lot of influence on how the decision is being made regarding bus frequency. Okay, so from here, you'll be able to see a lot of, information. You know, we have done a lot of… quite a bit of work on this already. So… If you are interested in this, you can… Go further. One important thing I want to highlight is that even though we're looking at a case study. Whatever solution, whatever argument we're going to make, can be scaled up to improve the bus service of the entire city. So, so, I hope you won't treat it as just a trivial case. Because there is the potential to scale up. Alright, whatever argument we make, It can… it has the potential to improve everybody's life. Okay, because everybody take buses, right? I mean, except the super rich, of course. Alright, the second one, is about, how we can There's the case for merging two bus stops. You know, that are pretty close, okay? Again, you can scale up the case. Even though this is just one case from my neighborhood, we can look through All the different geographical locations of different bus stops and see there might be other cases as well. The third one… It's about, how we could coordinate Different bus routes that may have overlap. Imagine there are two bus routes covering a big… Chunk of bus stops. overlapping. Then, if we coordinate the bus routes. So, the buses will come more evenly. Is that me? And that would actually improve the… Passenger experience. And for that to happen, we need to use simulation. Okay, we need to use some mass tools to demonstrate this could happen. Alright, so that's the first two, three topics all about the public transportation. Number 4… It's about, municipal solid waste charging scheme. Well, actually, the government has already shelved the scheme. We have no idea whether they're going to revisit it. But we have done some work on this already. We have done a little bit of a questionnaire, looking at how people might view or perceive this whole scheme differently. When we offer different information to them. And this is a very interesting case to explore, because By connecting public perception or a policy issue to how the government might tell their stories differently. So essentially, we're trying to look at how the government makes decisions about storytelling. about publicity. Okay, so that's actually a slightly different approach. to… Data governance and policymaking. But again, if anyone who wants to pursue this project. We already did some preliminary work. Okay, so you won't start from scratch, which is always… Something good, something reassuring. Hopping number 5? It's also related to, waste management. But this time, we are looking at a green ad community. Anyone aware of this? No joy, koi, koi koi. Green Net Community, okay? Now, again, In another course that I taught. Some students have done some work. And you can see there's a letter being published. On this topic. But the bottom line is that Based on… My students' research We believe that this screen and community project. It's a waste of money. Because if you consider the amount of money being spent. And the amount of solid waste being recycled. We can very hardly justify. The amount of money spent. Okay? Topic number 7… is, not so popular topic. You know, anyone who picked this topic… actually, I was talking with a computer science student, Andy. he will probably work on this topic, but anyone who wants to join Andy, This is your opportunity. This topic came from a decision made by the Hong Kong Observatory last Monday. If you remember, if you recall, what happened last… Not last Monday, the… not yesterday, I mean, the… the Monday of the previous week. So, if you recall. The Hong Kong Observatory actually broke its tradition, or convention. Instead of announcing the decision to hoist the number 8 signal, like, 3 or 4 hours ago. they basically announced the decision around, like, 7 or 8 PM. And say, we are going to hoist the number 8 signal until… at least until 11 a.m. the next… the next day. So the Education Bureau actually announced that schools will be closed. Not in the morning. Not, like, 5 AM. But around 7 or 8 PM, Now… If you look at the convention or prevailing guideline of the Hong Kong Observatory. They actually take a data-driven approach. They collect and monitor the wind Data, the wind speed data, from 30 different stations around the city. And they will only consider issuing signal number 8 when over half of the station. Gets the wind strength, or wind speed, Passing a threshold. Okay? So that's the conventional prevailing guideline. But… During the typhoon night, I was using the API, To monitor the wind data from the 30 stations, I found that not… over 15. Stations got the… passed the threshold. Actually, I built a website called Hong Kong Wind Monitor. And right now, the website is useless, because there's no special signal. But during the typhoon day, when number 8 is up, here it will show number 8. And then… Using the API, we are able to show the real-time wind speeds data from the 30 station. And you see how many of them reach Signal number 8. So, in other words. We're going to apply a data-driven approach to this specific case, and we're going to communicate with Hong Kong Observatory and ask them How can you possibly make the decision and prediction on the wind strength Like, 12 hours in advance. Okay? So, this would be a very interesting project as well. Of course, it might not be so popular, because people are very happy when they have an actual day off, right? I actually don't really have an actual day off. You know, when the number 8 signal was… was off, taken down around 1PM in the afternoon. You have to go to work and show up in 2 hours, at least that's what I did, you know, here at BU. So anyway, so that's, that's, number 7. The last one… Oh, oh, actually, I haven't done… I haven't talked about number… number 6. Now, number 6 is about flu shot. It's about the influenza vaccination, okay? So it's a… it's a public health issue. Actually, many years ago, I published a couple of letters on this. So, 2018… 2017, so you see, it was years ago. Now, if you look at those letters. And, you can see that there's already some, some, some data. You know, there's already some data. quantitative information, but it's very basic, because at the time, I'm just doing VIN counting. I haven't been talking with… with math colleagues, with Talia, so… But now, with our math model. And our new, toolkit, we're going to take a look at this again. Alright. And finally, Topic number 8 If you are not interested in any of them, the previous 7 topics, if you find, oh, not so, Exciting. That's Andy. Andy, you wanna say hi? You only do the Typhoon one, right? Okay, so anyone who wants to do the typhoon one, you talk to Andy. Okay. Now… If you are not interested in any of them, There's an open-ended option. Okay? So, what you need to do is to go to… data.gov.hk. And just take any data set. And see whether you are able to connect the dataset with a specific policy or decision made by the government. And you go from there. Okay? So that's the last one. Oh, I didn't put a link here. Now, the good news about this data.gov.hk is that again, there's all the API available, The different, providers. Of, the dataset. Okay? Now, anyone who wants to pick number 8, It's going to be, very open-ended. So, you have to be patient, because… You have to do a little bit more exploration. And probably also a little bit of programming. Okay? But, again, I think there's a lot of possibilities there. Great. So, any questions? So how about this? We have an early break, say, 5 minutes. We take a 5-minute break, so you guys can talk to one another, and maybe start forming groups, start looking at topics. And, once we… we're done. You know, hopefully you already got a team formed. or you can say hi to each other, you can reply here, you know, just take 5 minutes, and then Talia will start talking about math. Okay. Any questions? volcano. Okay. I think I should, share screen again. Supposed to share a screen. Okay. The last time, we, do some basic data visualization with the help of GitHub Copilot, so I'll briefly recap. Usually, when we, try to visualize data, we will follow these steps. So, first of all, we need to load the dataset into our environment. And then we can have a basic understanding of what variables are contained in the dataset. Then use the correct way to visualize different types of data. For example, if we have categorical data, we usually use these bar charts, or pie charts. And for continuous data type, we will use the histogram, or this box whisker Plus, to see the distribution. And we can use scatterplots to see the relationship between two continuous random variables, and then save visualizations to a directory. So last time, we used the, questionnaire data, for the, the, Municipal Solid Waste Data Questionnaire. So after we load the data, we usually have a look of what those, variables' names are. For example, the key variable in our dataset will be the support information. So one refers to strongly oppose the discharging scheme. Whereas 5 represents for the strong, support for the, charging scheme. So on and so forth. We have some variables recording the attitude of the public. Like, the preserved fairness of this policy, whether the government has good responsiveness to the public opinion while making this policy Whether this policy is helpful for reducing the waste. whether Hong Kong's waste management situation is severe or not. So this part is about the attitude towards this different aspect. Of this policy. And we also have other information, like where do the respondents leave. For each district, you know, there are 18 districts in Hong Kong, so for each district, we use a 0 or 1 variable to specify where this specific Respondent leaves. And we also record information like, what kind of, house type, or age, or some other demographic features of the, the respondents of the questionnaire. To see the whole list of the variables measured, we can use this info So, info. Then here, lists all the pos… all the names of the variables. So, last time when we, do the warm-up exercise, some error is due to the wrong variable name. So, for example, here we have this fairness. But if later, when the co-pilot generates the code, it may use, say, perceived fairness, then that will lead to some error, because the variable name in our dataset is fairness, but not perceived fairness. So… Pay attention to these variable names and make sure that they are correctly cited in the later visualization part. Okay, so then we move to the, visualization, visualize the categorical data. So, first, The most, the variable that we care most is the support level, so we can first show the frequency table. See overall, how many people are, say, support or neutral, so on and so forth. Because it's categorical, it's usually easy to use this bar chart, We can see overall, Most people seems to be, support in general. We can also use this pie chart. But personally, I still prefer this bar chart, because I think human eyes are still more good at Judging the height of this bus, rather than the areas of these sections in a circle. So, for example, for this neutral and support. Just in one glance, it's a bit hard to tell which piece is greater. So this is for, the, support level. And, of course, we have a pre-information support level, and also support level after the respondents reading two pieces of information. So if you still remember, we showed the respondents of the questionnaire two pieces of information. The first information is that two landfills in Hong Kong are approaching the limit. which reflects the severity of Hong Kong's waste management. And the second piece of information is a beneficial case of South Korean Where they implement the waste charging scheme around 30 years ago. And the good effect lasts to now, where their recycling rate is around 60%, whereas Hong Kong is actually around 30%, so basically double of Hong Kong's, figures. So, we want to test whether, after reading these two pieces of information, the level of support for this waste charging scheme will be changed, or whether it will be improved. And just reading from this graph, it seems that, the number of Respondents selects the strongly opposed decreased. From around 20 to, 10-something. And then for the second group, the opposed group also decreased a bit. And it seems that for these two groups, they moved to the support group. But interestingly, for the strongly support group, the number decreased after reading this information. So this is just the intuitive understanding from the visualization, but whether this change is is so-called significant. We will, talk about this, in the… in the last part of this, today's class. And then… and then we have the visualization of all other categorical variables, received, fairness, so on and so forth. So we can see, largely, people, regard that, this policy. Is… fair, so generally speaking, and also, generally speaking, they believe that this policy, at least is somewhat helpful for the reduction of waste. And, more people tend to believe that the severity of Hong Kong's waste management is somewhat severe. And the next visualization actually pay attention to the living districts of these, respondents. So overall, we have around 100 responses of the questionnaire. And this figure shows the frequency of participants by Hong Kong District. So for this type of graph. It's better to order the bus from the highest to the lowest. Instead of having those high and low, high and low bars. And from here, we can see that most of these participants of the survey are living in Colon City. Whereas we did not, distribute the questionnaire to someone living in the North District. With this information, we can also check Whether, those who live in different districts have seen the food waste bin. And if they have seen it, Whether they have used that. So for this figure, the x-axis shows different districts. Whereas the y-axis shows the percentages of these three categories. So there are 3 bars that, is very, high. Which reaches 100%. So you may guess that for those three categories, I would bet that the total number of participants living in those districts are quite limited, so that we can have this 100%. So, for example, One chai, Let me go back. So here, it turns out that there are only two participants from the, one-chai districts. And all of these two, or both of them. Have never seen the food waste a bit. So on and so forth. So there are some extreme values due to the very limited number of participants. But for other districts, like, I think Colon City. Which has the, largest number of respondents. Still, the percentages of never seeing the facility accounts for the largest percentage. Whereas for those who Who have seen the food waste bin. Have have. They used or not. So these are all descriptive, Information that we can, extract from our dataset. So instead of using numbers, hopefully these visualizations are more reader intuitive. And we also have one, fake. Continuous data, continuous variable, So I actually manually generate this, variable, because for the questionnaire. Actually, there's no categorical data, so I made this variable just to show you if there is a continuous variable, how we can use visualization. To, to, to visualize it, and also how to build relationship with other categorical variables. So for continuous variable, like the distance from home to the nearest recycling facility, We often use either histogram. Or this box whisker plot to show the distribution of this continuous variable. So maybe let's start from this, histogram. So usually, we will… Cut the range of the continuous variable into equally spaced intervals. So here, it's like, for every bin here, it's around 10 meters. Then, we can count how many, individuals Their, the distance between their home To the nearest recycling facilities falls into that specific interval. Say, between 100 to 110. And there are 5 respondents. The distance between their home to the nearest recycle facility is between 100 to 120. And for this highest bar, it shows the highest frequency. So most people in this dataset, the distance between their home To the nearest recycling facility is around 145 to 155 meters. And similarly, we can use this, box whisker plot to have a, Another representation of the distribution of the, this continuous type of variable. And the last part is to explore the relationships between variables. To say, if we want to know whether there's a relationship between the distance the nearest recycled facility versus the recycling effort. Intuitively, these two… Should be negatively correlated. It means that with the increase of the distance from home to the nearest recycled facility. Then the recycling effort will decrease. Because… to recycle, The responders need to walk so far away to put those items into the recycle bins. So if that's the case, that will demotivate the participants to… to have this recycling behavior. Or pay you more recycling effort. So it seems that when the distance is here, like, only 50 meters, the distance is quite close. The recycling effort, is high. Whereas with the increase of the distance, you can see, gradually the recycling effort drops. Right. Because in this questionnaire, this recycling effort is measured by 1, 2, 3, 4. So there's no, middle… there's no value falls between these, this, integers. So it just dropped, like, going down… Going downstairs. With the increase of the, distance. between the home to the nearest recycle facilities, it turns out the recycling effort is gradually decreased. So there is a negative Association between these two. And at last, some of these, figures might be, informative for the future study, then we can save those figures. So the last section is used to save the desired, The plot into the designated district directory, so that later we can make use of it. Okay, so that's the recap of the, visualization tasks. And you can find this notebook on Moodle. Oh, I see. This is my… This is our Moodle page. So we have a section now called Jupyter Notebook. The data visualization notebook can be found there, but if you click it, it will be shown as a JSON file, but don't worry, if you save as… Save it as… another file, it will be in a Jupyter notebook. And you can put it in your working directory, and open it using the… use the VS Code, and everything will be fine. Okay. Okay, so… No, I will. Any question? Any questions? So, basically, the Jupyter Notebook provides us an opportunity to Handle the data. By writing some, Python codes, programming codes. That's what we experienced last week. You know, some students struggle a little bit to set up VS Code and everything. If you are not using a mobile computer. There's alternative, which is you can go to Google Collab. And, you can load the Jupyter Notebook file over there. we can upload it, right? We can upload a file to Google Drive. Alright, so, so what you need to do is download the Jupyter Notebook file and upload to Google Drive, and then open it using Google Collab. But then you won't have access to the GitHub Copilot on your site, so you have to go to another chatbot, like Poe or Grok. And… but you can still use AI to do the coding for you. Alright? So, we're gonna continue taking this approach. So, we're gonna have some data, and the data will be in, like, CSV or some spreadsheet. And those data is more for, like, machine. But if we want to generate something visualized for humans to understand, then we need to write a computer program. Okay, so that's what Talia showed us. So the programming codes that AI help us to generate can be used to visualize. And can be used to visualize the data, and later on, it can also be used to run some math models. So that's why it's really important to learn how to set things up. But as you can see, we don't really need to learn any syntax or programming, all we need is just, talk to AI, and we… if we ask the right question, we'll get the… get the answers we need. Okay, so… I think you have heard a lot of math models this morning, and I want to clarify that we will only introduce two so-called math models. The first is regression, which will be discussed in a while. And second is optimization via simulation. So we'll only talk about these two so-called math models. We choose these two because they are commonly used, and they are especially fit for, solving the problem as introduced, like the bus transportation type of problem, and the, public opinion type of problems. So these are the tools to help you to solve the, certain, questions you want to, you want to, explore. So in the next 40 minutes, I will talk about regression and see how it can be used to study the relationship between the level of supportness for this policy and other potential influencing factors. And for the last class for today's course, we will work the in-class exercise. So we have already recapped the week two, the visualization part. And this case study one is to talk about regression and to explore using linear regression models to analyze public willingness. And correspondingly, we will make policy recommendation. So before I start, may I ask you to show me whether you have learned the regression before? If yes, do you mind showing your… Okay, okay. I will not go to… go to too far. I will only focus on the basics today. So here shows the structure of today's case study. I will briefly introduce the background of this policy, and also about the solid waste. And then talk about the, study design, which may be Used later in your own, group project. And then about the methodology, and then results interpretation, and some recommendations. So, background. Talking about the solid waste, actually, there are, different subtypes under this solid waste concept. So the, municipal solid waste is just one of them. And under the solid waste, there are overall construction waste and special waste. So in 2023, Hong Kong's waste amount for this municipal solid waste is around 3.97 million tons. Per year. And these… MSW. Accounts for around 70% of all the solid waste. So, it is the largest proportion of solid waste. And under this municipal solid waste, it can be further categorized to domestic waste, which is basically the waste generated by a household. And commercial and industrial waste. So those are generated, say, by the restaurant owner's business. And the existing situation of the waste management is severe. As you, you have known that two landfills are projected to be exhausted next year. And this policy was, proposed a very long time ago. The principle is the so-called polluter pays. Which requires the weight producers to bear the cost of disposal, Based on the quantity generated. So, the larger amount of waste The polluter generated the large amount of money they had to pay. So hopefully this policy will incentivizing waste reduction and recycling. And this policy was intended to apply to all sectors, including both the residential and non-residential. Then, we move to the study design. So, on the right-hand side, there is a flow chart. Overall, we will start with some research questions. So, we are starting from the questions that we want to investigate. So based on this specific questionnaire, there are, I think, at least two questions we want to answer. The first question is, Regarding the, support level of this charging policy. What kind of factors will influence the support level? If you still have an impression of the questionnaire, there are many questions measuring different… from different aspects. Of the potential influencing factors of the support level. For example, we have the attitude level, or attitude perspective questions, like the perceived fairness. We also have the knowledge perspective. We have some questions to test people's knowledge about recycling. Say, what kind of material can be recycled? So these are knowledge. And we also measure some behavioral level variables, like what's the frequency of recycling. How much effort do you pay on recycling? So these type of questions measuring the behavioral level of the respondents. And we also record the, demographic factors. Right, like the age, age group, or household income, or education level. So all of this, wearables. Can be the influencing factors of the level of support for this waste charging scheme. So this is the first question among all this, Variables we collected. Which one? Or which ones? Are associated with the level of support. And the second question is, we do have that pre- and post test, right? We show the two pieces of information and want to see whether this information will have certain effect on people's level of support. So we want to know whether this kind of information is somehow effective To change the respondent's perspective. So these are the two, Research questions that guide the design of the questionnaire. So starting from here, there will be two branches. The first is the, questionnaire construction. Which is this case specific. We may have other ways to collect the data. Say, we may have interview with some people, and during the conversations, we can have a In-depth understanding of why people believe in those ways. So here, we just, use questionnaire to put the questions that we're interested, On the… on the questionnaire. And on the other hand, we also need to consider our sampling methods. So this means that It is not feasible to ask this set of questions to all the individuals in the populations that we are interested in. So, for example, here, if we consider the population as all the Hong Kong adults. Then we know it's impossible to… to distribute the questionnaire to every single adult. So we have to… Select a subset from this population. So that it is feasible for us to, To give the questionnaire to only a subset. of population. And in that case, we call that subset a sample. Then, how can we select sample from the population? If we just… If we can't, Select Sample. Randomly. From the population, then somehow we can guarantee that those samples are representative of the whole population. Whereas if we just, follow certain criteria. Say, these peoples are relatively easy to approach. So, I know some friends, and I have families. I can find my families to fill in the questionnaire, or I can find my friends, and I can ask my friends' friends, so on and so forth. If we… Collect the opinion from From the, from the people in this way, then these kind of, sampling may have certain bias. Which means it cannot fully represent the population. So, for example, in our case. If you look at the, the education level of the respondents to our questionnaire, you will find that most of the people Holding master's degree or above. So this is a sign showing that this response I mean, the participants of this questionnaire has already deviated from the very typical profile from our, Hong Kong population. So that will inevitably introduce some bias to sample. But to make it feasible, maybe we can start with this. This sample that are easy to, access. And the other question is that, how many samples do we need? Right. Regarding the assembling methods, How do we, find the subset from population? Then, how many samples do we need? Actually, if we can really achieve this randomly sampling from the population, there is a very regious way to calculate the number of samples needed based on certain, statistical concepts. But if it is… If it is the case that we cannot strictly follow this random select samples. Usually the sampling… The sample size we need is… I think between 100 and 400, it will be enough. Because this number will ensure that the margin of error is less than 10%. So, generally speaking, around 100 should be okay. With these two, steps done, We have constructed the questionnaire Also decided, where we will sample. The population, and how many samples we need. Then we can move to implement our data collection. And then we can do, basic data processing, like cleaning our data and do some exploratory data visualization. Then analyze it, interpret the results, and then write the report. So that's the overall, study design for, the… This waste charging scheme. P. So then, what kind of tool can help us to determine the relationship between the level of supportness and some potential variables, like the attitude, their behavior, their knowledge, and their demographic factors? So in this case, Y represents for the level of support, which is the response variable. And X is the, possible explanatory variable. And this linear regression can help us to determine whether there is any association between pair of variables or among a set of variables. So when we use linear regression, actually there are assumptions. So first, this response variable is often to be a continuous variable. For example, the amount the participants are willing to pay for the charging scheme. But you may argue that, well, for our questionnaire. Our, variable of interest is the level of support. And we only set 1, 2, 3, 4, 5, right? So it is not a… continuous response. Well, that is the case. But given that our sample size is around 100, is large enough. And from the previous visualization, we can observe the shape of those responses. So the shape of these responses on the left-hand side is not that deviated from a belt shape. Then it is still okay to start with regarding the level of support as a continuous variable, and use this linear regression. To see whether the level of support is associated with other variables. we're interested. And another assumption is that The relationship between X and Y's are linear. Which means that… If we plot X and Y's, on the… on a plane. So this is X, this is Y. We have some data points. And if we fit a line. If we… if we have write it in this way. It means that the relationship between X and Y can be described using a straight line. So that is where this linear comes from. It's not a… Curve or something. So we assume that the relationship between the, the level of support and other variables Having this straight line type of… Relationship. And… Then how do we know whether there is association between them? Well, we can use the data we collected to estimate the effect. Specifically, to estimate what beta 0 is and what beta 1 is. So we may have Y… equals to 1 plus X. So then we have a relatively definite relationship between X and Y. And we know that if that is the case, Then… this… this one, maybe I use two here. So if it is the case that use our data. We can estimate Beta 0 is 1, and beta 1 is 2, It means that with one unit increase, Of this, say, age. Then, on average, the level of support will increase 2 points. So that's the, interpretation of this There's Beta Zero and Beta 1. So using this, we can quantify the expected change in this response For a one-unit increase in an explanatory variable. If we have more than one Explanatory variable here, like, we have age, we have income, we have other attitudes or behavior level variables. This coefficient will be, on average, how the level of support changed with one unit increase of this specific S, While holding other variables the same. So that's the, interpretation. We will go to the Jupyter Notebook to, to look at examples in a while. But before we go there, I would like to highlight that. Although we have a model here, and why it's called response. This relationship does not imply causal relationship. So the conclusion is not like, the, The, say, the knowledge, the knowledge level, will cause The increase… the higher level of knowledge will not… is not causing the increase of the level of support. So for regression, it only shows the positive or negative association. Between the variables, rather than causal. Because for causal, it has a strong direction, and we need further conditions to confirm that direction, which is more complicated than a single regression model. We will look at the, examples, of using the regression model to our, waste charging scheme. On Moodle, there is a Jupyter notebook. I think it's called the regression models. Mmm. Let me see… I think you can find it… Here. So, on Moodle, the Jupiter notebook session, there is a week 3. I think if you download it and open it. Using VS Code, you can see the same screen. as what I'm showing… No? So although there are other two types of regression on the slides, and also in this notebook, I think today I will only show the content related to linear regression. I think the overall flow is the same. We first need to ensure that all relevant libraries are imported. And then we load the dataset into our, environment. Then we can have the head to see the variables, and also the first 5 rows. Then, I actually do some recording to, edit my variables. So in the original data set, the income is coded using binary variables. For example, if the individual, their household monthly income range is between $15,000 to $30,000, Then there will be 1 for that variable. And if the individual whose household monthly income does not fall into this range, there will be a zero. In that cell. So originally, there are these five, variables. And we use 0 or 1 to specify which category the individual falls into. But now, I want to recode it. To make the household monthly income to become one single variable. And I will use the midpoints of this, salary ranges to represent the, the household, the concrete household monthly income. So after this, step, There will be a new variable called income in the dataset. Which can only take these 5 values. So it, it shows the, Somehow, the household monthly income for the individual. I did the similar things for the educational level. I, recoded them from the binary 0 or 1. variable to… A new variable which… which is called education, to show the, Their education level from the, Shortest to the longest. And you can see, as mentioned. The largest category are those who have master's degree or above, which Which is obviously not representative of the population. Question. Even after Save As. Let me see… Yeah, you can right-click… actually, you can right-click the link. Can you go back to the Moodle? the… Let me see… So… You see here, in the side menu, we have Jupyter Notebook, right? So, if you… If you right-click here, Week 3 Regression, and save link. Save link S. Alright, save link as… And this is probably the easiest way. When you click on Save Link S, you will get the window to save the file as a Jupyter Notebook file. Alright. So, once you save it, you need to… move the file over to the folder where you open your VS Code. Alright. Well… Either you find a folder through this file navigation system, or you open that file in your… File Explorer, and then drag and drop over to the VS Code. So, somehow, you need to move it. So you will be able to open it using VS Code. If you are using your tablet computer. then you have to, use Google Collab. So then you need to… go to a browser, and log in, go to Google Collab, and then in Google Collab, there's an option for you to upload a file. So maybe I show you quickly… So, if you go to collab.google, you can… You can… Actually, what you can do is… Go to Google Drive first. Okay, so… you could… Let me see, locate… Is there a way to upload? Yeah. So, when you open Notebook. You can go down here, there's upload, so for students who want to use you know, Google Collab. And then you go to here. So this is how you select a file, and then you open it. Alright, so… this is another way to open the file. But the limitation of using Google Collab is, it does not have a… you know, innate, AI yet. or even if it will use Google Gemini, but in Hong Kong, we can't use Gemini without VPN. So, anyway, so this is another way to access, but then you need to go to poll. you have to go to poll and start, you know, copy and pasting everything. Okay. But, I mean, actually, Google Collab can be kind of a cleaner UI or interface, but as I said, you, you will have to copy and paste and use import or something. Anyone needs more help accessing the file? Okay. So, I also did a similar thing for age. Originally, the, HR… Age groups are coded, again, using binary. Whether or not the individual age falls into different categories, but now I use an age variable. To, use the midpoint of those age range. To… to represent the, age for… for… for the individuals. And it turns out that in, our respondents. Most respondents' age falls between 35 to 44. Okay, so finally, we get to this linear regression part. We will start with the so-called simple linear regression, which means we only have one exploratory variable. So the support info will be our response, and the perceived fairness will be the exploratory variable. And, if you… So previously, we, Nevermind. So I… here, I, made one more step to prepare my data. I used the original fairness, which is scaled from 1, 2, 3, 4, 5, to minus the average value of fairness. In other words, I shifted those perceived fairness From 1, 2, 3, 4, 5. To make it centered to the average perceived level of fairness. So here you can see that originally, for fairness, the largest value will be 5. And the smallest value is 1. And on average, the, perceived fairness is 2.94. But after I center it, it means that The original perceived fairness lower than this 2.94 will be negative. And where those, fair… level of fairness greater than this 2.94 will be, Will… will be greater than zero. So I do this centering because in our linear regression model, so we have this Y equals Y equals to… beta 0 plus beta 1X, So the interpretation of beta zero is that when X equals to zero, what is the average level of support, right? But in the original scale, for fairness. the value of X can only take 1, 2, 3, 4, 5. So… X cannot take 0. So to make it, easier to interpret, to interpret I first center the fairness. so that I, I shift them. 2.94 units. So… The average perceived fairness will become zero. So then the interpretation of this beta zero becomes… if… Individuals takes the average level of perceived fairness, then what is the average level of support? That will be the meaning of this beta zero. Then, we can, run this, linear regression. So to run this, you just… Type into the, type… prompt into this chat box with, Copilot. you can tell… tell it that, I would like to use, write code to use fairness centered as explanatory variable, and support info as the response variable, fit a simple linear regression. And the code will be, generated. And there will be some output. The key output is here in this table. It's called Model Summary. So this is the first, part of output. And usually, if we are doing simple linear regression with only one exploratory variable, then we can have a plot to show the relationship between the two. And then we can focus more on this model summary. So our, to tell the exact relationship between the level of support to fairness. We focus on the estimate of those Beta 0 and Beta 1. So I think at least we pay attention to two columns. The first column is the coefficient, which are the estimates of beta zero for the constant. And the estimated value for the beta 1. So the beta 1 represents the slope of this straight line. Whereas the constant is 2.8-something, Shows the, shows the, the, the intercept. Yeah, when X equals to 0, so this value should be the 2.83. So this is where the, The constant value comes from here. So this is the first column we need to pay attention about the estimated value for the beta 0 and beta 1. And another column is about this P. P is greater than this absolute value of T. So these are p-values. The p-values, you can see, they are very, very small. 0.00. Basically, as long as it is less than 0.0001, it will show as this 0.00. So if this value is very small, it means that, our coefficient is significantly different from zero. So this means that, our estimations, they are, They are significantly different from zero, meaning there is indeed certain association. Between the perceived fairness and the level of support. So here, I actually have a detailed explanation for the meaning of p-value. If you're interested, you could have a read… you could read it in a detail. I think, if you're not from the, do not have quantitative background, then just remember that if p-value is less than, say, 0.05, which is a commonly used threshold. Then we can, make the conclusion that these coefficients They are significantly different from zero. from population. So there's indeed association. Another thing is that we can pay attention to the sign. Of this coefficient, whether this is positive or negative. So once there is an association, whether the line is in this way, or the opposite on the opposite direction. So that will suggest, the… the direction of the association. And then we have this R squared. So the R-square is, a matrix to describe how much variation of the level of support in our sample. Can be explained by this fairness. So we know that in our sample, some people support this charging policy, whereas some other people against, right? There are some variations. For this opinion. But what kind of, things can explain this variation. Where does this variation come from? This R squared can be regarded as the, the extent that The variation of the opinion. Towards this charging policy. To how much extent can be explained by this level of perceived fairness? And it seems that around 52% of the variation of different opinions can be explained by the perceived level of fairness. So I think for, social science, this… this R squared is, is moderate. It's moderate to good. Yeah, I actually put all the, detailed explaining, in the later part. So, once we estimate this beta 0 and beta 1, We can, have certain predicted support. So suppose there is, an individual whose perceived fairness of the policy is 3, Then, we can, predict that their support level for this waste charging Scheme will be around 2.87, which is slightly below the neutral. So if someone whose perceived fairness is at the neutral level, then their level of support will be less than the neutral level, so more towards the against. However, if someone's fairness rating is as far… as high as 4, Then, we could predict, on average, their support level will be 3.66. Which means… If the person who's received fairness is a, fair. Then… The level of support will, will tend to be, higher than neutral. So, tend to support this policy. So this shows how higher the fairness ratings associated with meaningful higher support levels. So we can actually quantify, with one unit change of the received fairness. How much, will the support level will increase, so we can quantify the, the value. And a counterexample is the next simple linear regression, where we use this support information to regress on the distance. As mentioned, this is a fake variable, so it measures the distance between home to the nearest recycling facility. So, intuitively, there's… There would be no association between the two. Because the… The distance is… is fixed, So… I'm not sure whether this will affect support level, but we can use, again, simple linear regression to test the association between the two. And remember that we focus on the coefficient column, and also the p-value column. And it turns out that For the coefficient in front of this distance, so this is the beta 1, estimated beta 1, It's 0.0034 with insignificant value, which means that this coefficient It's not significantly different. from zero. So that means there's no significant association between the distance from home to the nearest recycling facility versus the level of support for the charging scheme. And, consistently, the R-square is very small, it's only, like, 2%. Versus previously, the level of fairness. Explan around 50% of the variation of level of support. So we can still do the visualization. And then… You may wonder that, well, we paid so much effort to collect information from different perspectives, right? From the attitude, to belief, to knowledge, to demographic features. Then, how about those variables? Well, we can use the multiple linear regression to put all the collected variables in the model. And see, when controlling all these variables from… all these potential influencing factors from different perspectives, whether there will be association between, among these variables and the support level. So again, for all this, these variables, I do the centering. And I put all the features here, and I just ask Copilot to help me to run the multiple linear regression with all of the features I collected. And it turns out that If you look at the p-value column first. Only the first 3 rows, which represents the, beta zero. perceived fairness, And also the, the level, the government. Respond to public opinion when making this policy. Only these two factors are significantly associated with the level of support. Whereas all the other, variables we collected. Are not significantly associated with the level of support. Then you may think, well, then why, at the very beginning, we consider all of them, all of them? So, then how do I decide what kind of variables to include, right? There are so many potential influence and factors. Well, these are actually… come from, the, reference. So some other scholars have, Done certain, studies. On the influencing factors of the level of support. So based on their studies, they all consider… they consider all of these, variables from different perspectives, and that's the reason we take their research output as the reference. And, kind of verify their conclusions, and then to include these variables. So here we have a detailed interpretation of this result. And the last thing is, we can perform the, so-called forward selection. Using, these steps, To see what really matters among a lot of potential influencing factors. So we will start with, a model with no explanatory variable. And then, we add We pick, randomly pick one. to the variable, and… and one by one, actually, to… to see, their p-values. So whether they have Significant association with the level of support. And we will keep adding the potential explanatory variables into the model. Until there's no remaining variables having p-value less than 5%. Then we see, we observe, which mod… which variable, or which variables are maintained. So to… to do this, you just, again, type Write code to implement forward selection for multiple linear regression with features 1, 2, 3, 4, 5. Then you will have this output. And after running it, it turns out that We got the same results, so only the, the responsiveness to the public opinion and preserved fairness. Are significantly associated with the level of support. And because… The fairness and the responsiveness, their scale are the same. Those 1, 2, 3, 4, 5. Although I have centered them, their scales are the same. We can compare the coefficient. So which one is bigger? And it turns out that the government consideration, the effect. On the level of support is slightly larger compared to the level of fairness. So later, when we prioritize these, two influencing factors. We may prioritize this government consideration. Okay. So let me jump to the, I think I have recommendations. At the end? Oh, on the slides. Let's see… I will jump into the, conclusion based on the regression results we have. So first, a quick summary of How we do… how we did this survey, what's the sample size, how we collect the sample. So we have the regression models on public support for Hong Kong's waste charging policy based on 97 non-probability samples. So we did not follow the very strict random sampling. We… we had to tell. And then, we found that the key predictors of the level of support is perceived government responsiveness. So we have the, coefficient, which is about 0.4, 5-4. P-value is very small, meaning that it's significant. And we also find the preserved policy fairness, also significantly associated with level of support. Coefficient is around 0.42. And overall, model fit is around 61%. It's adequate and exploratory. But there are certain limitations. We have small, non-representative samples, which risk selection bias. So, it has limited general… Ability to generalize our, conclusion. It needs further validation via larger probability-based surveys. So, correspondingly, we will have the following policy recommendation. For the last one. The last one I haven't showed yet, but let's focus on the first two that are related to the regression results. So, regarding the, improving the, public support for this policy, we may prioritize the responsiveness For example, we can, suggest the government to enhance public engagement, say, through town hall meetings, or some online… open some online portals to collect, the opinions. To build more trust. And potentially boosting wilderness by 0. Improvement. So this… This quantity is based on the coefficient in our regression models. But of course, this is pending validation in the larger probability-based samples. And then we could try to address fairness, secondarily. to try to implement some transparent, equity. For example, Government can consider, address the low-income subsidies for implementing this, Waste charging policy. And regarding the last point, it's actually, related to the second research question, which is about whether the two pieces of information Significantly change the, the level of support for the respondents. So to answer this question, we actually need a, test. a statistical test? So, in the notebook, I think I put it… I did not put the results here, so we need a small statistical test to see whether the median of the two responses changed. And it turns out that there is significant change of level of support before and after reading the two pieces of information. So based on that result, we can have the corresponding recommendation that Preliminary evidence from the pre-post analysis suggests that targeted information on the waste crisis severity And also, policy benefits may enhance the public support for the, waste charging scheme. So that's basically the main content for this case study. It shows how we can use, linear regression to link up the level of support to two specific influencing factors, the perceived fairness and the government's response to the public opinion. Highlighted these two influencing factors. Maybe, maybe I just make a very quick comment. Please come in. So… what Talia suggested and explained earlier is we run the data, we run the model, and we analyze the data, and we reach this particular conclusion. And then the next step for anyone, you know, the team to work on this project is to go back and to take a look at how the government actually explained this policy. Alright, so we have to go back to the Legislative Council documents, any media reports, any documents that we can get from the Hong Kong government website, and see the discourse. Now we can start doing a little bit of the qualitative analysis. To see how the government staff was telling the story about this, waste charging scheme. And, based on the insights we gained from this Quantitative analysis of the survey results We can then assess To what extent the government staff is telling the story in an effective way? And of course, we can also approach the government staff and say, look, when you, you know, decide how to tell the story, how to promote or explain the policy. Have you taken into consideration any data about the perception? Have you run any, survey or engaged in any consulting company that's actually trying to gauge the public opinions on this policy issue? So that's something we need to do. Together with the quantitative analysis, then we'll be able to offer more recommendations regarding how the government staff can do a better job to promote. the charging scheme. And frankly speaking, the government failed spectacularly over the charging scheme, because they've been talking about charging scheme for at least 10 years, or even 20 years. But then, at the end of the day, they still couldn't do it. Yeah, I think that Secretary of Environment was about, you know, almost get fired because of this. But again, you know, we are facing this crisis of solid waste management. But, yeah. I hope… I hope that a little bit, kind of, to supplement what Dr. Wu just said. Okay, I think we can have a break, and after that, we can work on the in-class exercise. What's a deadline? Absolutely. Central demo. Good morning. You call your phone. Oh. No, that one, what is it? They're gonna be doing it before. You want an email, that's not good. You can download it. Oh, honey. No wonder. Thank you. W2… No, but you… So, just a quick comment. Well, boast, in-class access as last week, you get familiar with the VS Code, and also in-class access this week. Well, actually, last week is not called in-class exercise, but this week, the in-class exercise actually counts for… for some percentage of the course. you can use Copiler, you can ask Copiler to write the codes for you, and then you run the codes in the Jupyter Notebook file. No problem with that. But remember, You don't want to just, like, mechanically copying without actually looking at the codes. and the results, right? I mean, the syntax of the codes may not matter that much to you. But the programming codes at this level is actually human-readable. You can actually take a look at the codes and try to figure out what it does. If you still don't know what the codes do, you can still ask AI to explain it a little bit more. You see what I mean? So, even though we are not really learning the syntax, I don't want you guys just copy and paste and everything without knowing what you're doing. So, you need to know what you're doing, okay? That's very important. And think of this as an opportunity to learn how the whole… A regression and everything is working. Right? Because you need to know how things work. In order to later apply the same tool. To your… to the problem you are working on for the project. Okay. Now, I haven't prepared a customized chatbot for you. But for now, if you want to get some extra help from AI, either you can use Copilot. Or you can also go to… Dumb… a general AI platform of our university. If you use, our university's, generated platform, what you can do is actually attach… I'm not sure whether it would be… It can actually, hold on. So, what you can do is… To download the regression, the… the Jupiter Notebook? There's already one here, maybe it doesn't, take the… the Jupyter Notebook file format, so you can, change it to text. And once you change to text, it will become just, plain text. And then you will be able to, Attach the file over here, and then you can ask any question, right? Something like… Explain. Excuse me. What? This file is developed in bullet points. Assuming… No. On each. Fence. Thank you for me. Something like that. Oh, okay, so you have… It cannot prof… Hmm. Or you can just copy and paste. Oops. the, Those universities' AI platform doesn't work, sorry. Well, you can, you can always ask the co-pilot, okay? A quick reminder, when you use CS code, you have to save the file. Remember to save your file. It could be CTRL-S or CMD-S. Okay. Alright, so, we have to, we have to leave this classroom very soon. The… you can submit the in-class exercise to Moodle, later today. And if you have any questions, you can, I think we can go down to, like, the sixth floor, there's some area, and we can take a few more questions over there. All through WhatsApp. Okay? I'm gonna wear a snorkel again. Wow, thank you. Take a time. Creepy, yeah. Legal. The link will be rolled with you. And supposedly in class mode, they extend the deadline, or they kind of… Definitely optional. That's a lot of them. bombing Ukraine. Let me tell them. Okay, so we have to go. Maybe let's just go to 6th floor, and we can take a few more questions over there.